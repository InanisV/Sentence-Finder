{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "from peewee import *\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "PATH = r\"/root/course/cs332/gutenberg/book\"\n",
    "NEW_PATH = r\"/root/course/cs332/gutenberg/contents\"\n",
    "CHUNK_PATH = r\"/root/course/cs332/gutenberg/chunks\"\n",
    "BASE_PATH = r\"/root/course/cs332/gutenberg/\"\n",
    "\n",
    "token_dic = dict()\n",
    "chunk_token = dict()\n",
    "chunk_count = 1\n",
    "\n",
    "file_count = 2661\n",
    "file_length = dict()\n",
    "\n",
    "gutenberg_db = PostgresqlDatabase('<data base name>', user='<user name>', password='<user password>',\n",
    "                           host='<host name>', port='<port number>')\n",
    "\n",
    "class BaseModel(Model):\n",
    "    \"\"\"A base model that will use our Postgresql database\"\"\"\n",
    "    class Meta:\n",
    "        database = gutenberg_db\n",
    "\n",
    "class Book(BaseModel):\n",
    "    author = CharField()\n",
    "    title = TextField()\n",
    "    context = BlobField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2714/2714 [4:02:23<00:00,  5.36s/it]   \n",
      "100%|██████████| 1/1 [00:01<00:00,  1.84s/it]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.29s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.84s/it]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.28s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.36s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.22s/it]\n",
      "100%|██████████| 1/1 [00:15<00:00, 15.48s/it]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.56s/it]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.17s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.17s/it]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.37s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.62s/it]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.38s/it]\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.16s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1728.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1258.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1339.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=2662, j=2733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "========================\n",
    "    DATA PREPARATION\n",
    "========================\n",
    "\n",
    "In this stage, the book context will be extracted and stored into the database.\n",
    "'''\n",
    "\n",
    "Book.create_table()\n",
    "\n",
    "def rewrite_file(doc, new_doc, doc_idx) -> bool:\n",
    "    _title, _author = str(), str()\n",
    "    begin, end = -1, -1\n",
    "    \n",
    "    with open(doc, \"r+\", encoding=\"ISO-8859-1\") as _file:\n",
    "        _content = _file.read()\n",
    "    _pattern = re.search(r\"Title: (.*)\\n(.*)\", _content)\n",
    "    if _pattern != None:\n",
    "        _title = _pattern.group(1)\n",
    "        if any(c.isalpha() for c in _pattern.group(2)):\n",
    "            _title += \" \" + _pattern.group(2).strip()\n",
    "#         print(_title)\n",
    "    else:\n",
    "        return False\n",
    "    _pattern = re.search(r\"Author: (.*)\", _content)\n",
    "    if _pattern != None:\n",
    "        _author = _pattern.group(1)\n",
    "#         print(_author)\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    with open(doc, \"r+\", encoding=\"ISO-8859-1\") as _file:\n",
    "        text_enum = enumerate(_file.readlines())\n",
    "        for i, line in text_enum:\n",
    "            _pattern = re.search(r\"(^\\*\\*\\*START OF (.*)$|^\\*\\*\\* START OF (.*)$)\", line)\n",
    "            if _pattern != None:\n",
    "                begin = i + 10\n",
    "                break\n",
    "        for i, line in text_enum:\n",
    "            _pattern = re.search(r\"(^\\*\\*\\*END OF (.*)$|^\\*\\*\\* END OF (.*)$)\", line)\n",
    "            if _pattern != None:\n",
    "                end = i - 3\n",
    "                break\n",
    "    if begin == -1 or end == -1:\n",
    "        return False\n",
    "#     else:\n",
    "#         print(begin)\n",
    "#         print(end)\n",
    "    \n",
    "    with open(doc, \"r+\", encoding=\"ISO-8859-1\") as _file:\n",
    "        _target = _file.readlines()[begin:end]\n",
    "    with open(new_doc, \"w+\", encoding=\"ISO-8859-1\") as _file:\n",
    "        _file.writelines(_target)\n",
    "\n",
    "    with open(new_doc, \"rb\") as _file:\n",
    "        data = _file.read()\n",
    "    book = Book(author=_author, title=_title, context=data)\n",
    "    book.save()\n",
    "    return True\n",
    "\n",
    "    \n",
    "def rename():\n",
    "    i = 1\n",
    "    j = 1\n",
    "    for root, dirs, files in os.walk(PATH, topdown=True):\n",
    "        dirs.sort()\n",
    "        for filename in tqdm(sorted(files)):\n",
    "#             if i == 4:\n",
    "#                 break\n",
    "#             print(filename)\n",
    "            abs_path = os.path.join(root, filename)\n",
    "            abs_new = os.path.join(NEW_PATH, str(i))\n",
    "            if rewrite_file(abs_path, abs_new, i):\n",
    "                i += 1\n",
    "            j += 1\n",
    "    print(f\"i={i}, j={j}\")\n",
    "\n",
    "rename()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2661/2661 [04:30<00:00,  9.83it/s]\n",
      "100%|██████████| 365/365 [15:07<00:00,  2.49s/it]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "===================================\n",
    "    INVERTED INDEX CONSTRUCTION\n",
    "===================================\n",
    "\n",
    "In this stage, the inverted index list is going to be built and stored in a byte file.\n",
    "'''\n",
    "\n",
    "def SPIMI_invert(doc, doc_id):\n",
    "    global chunk_token, chunk_count\n",
    "    with open(doc, \"r+\", encoding=\"ISO-8859-1\") as _file:\n",
    "        _content = _file.read()\n",
    "    _content = _content.lower()\n",
    "    word_tokenizer = RegexpTokenizer('[A-Za-z]+')\n",
    "    terms = word_tokenizer.tokenize(_content)\n",
    "    for item in terms:\n",
    "        if sys.getsizeof(chunk_token) > 1000000:\n",
    "            chunk_file = os.path.join(CHUNK_PATH, f\"{chunk_count}.txt\")\n",
    "            with open(chunk_file, \"wb\") as _file:\n",
    "                pickle.dump(chunk_token, _file)\n",
    "            chunk_token.clear()\n",
    "            chunk_count += 1\n",
    "        if item in chunk_token:\n",
    "            if doc_id in chunk_token[item]:\n",
    "                chunk_token[item][doc_id] += 1\n",
    "            else:\n",
    "                chunk_token[item][doc_id] = 1\n",
    "        else:\n",
    "            chunk_token[item] = dict()\n",
    "            chunk_token[item][doc_id] = 1\n",
    "\n",
    "def SPIMI_merge():\n",
    "    global token_dic\n",
    "    for i in tqdm(range(chunk_count)):\n",
    "        chunk_file = os.path.join(CHUNK_PATH, f\"{i+1}.txt\")\n",
    "        with open(chunk_file, \"rb\") as _file:\n",
    "            _dict = pickle.loads(_file.read())\n",
    "        for item in set(list(token_dic.keys())+list(_dict.keys())):\n",
    "            if item in token_dic and item in _dict:\n",
    "                for doc_id in set(list(token_dic[item].keys())+list(_dict[item].keys())):\n",
    "                    if doc_id in token_dic[item] and doc_id in _dict[item]:\n",
    "                        token_dic[item][doc_id] += _dict[item][doc_id]\n",
    "                    elif doc_id in _dict[item]:\n",
    "                        token_dic[item][doc_id] = _dict[item][doc_id]\n",
    "            elif item in _dict:\n",
    "                token_dic[item] = _dict[item]\n",
    "\n",
    "def tokenize():\n",
    "    global chunk_count, chunk_token\n",
    "    if not os.path.exists(CHUNK_PATH):\n",
    "        os.makedirs(CHUNK_PATH)\n",
    "    i = 1\n",
    "    for root, dirs, files in os.walk(NEW_PATH, topdown=True):\n",
    "        dirs.sort()\n",
    "        for filename in tqdm(sorted(files)):\n",
    "            abs_new = os.path.join(NEW_PATH, str(i))\n",
    "            SPIMI_invert(abs_new, i)\n",
    "            i += 1\n",
    "    chunk_file = os.path.join(CHUNK_PATH, f\"{chunk_count}.txt\")\n",
    "    with open(chunk_file, \"wb\") as _file:\n",
    "        pickle.dump(chunk_token, _file)\n",
    "    chunk_token.clear()\n",
    "    SPIMI_merge()\n",
    "\n",
    "tokenize()\n",
    "\n",
    "with open(os.path.join(BASE_PATH, f\"inverted_index\"), \"wb\") as _file:\n",
    "    pickle.dump(token_dic, _file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518339/518339 [00:20<00:00, 25332.38it/s] \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "================================\n",
    "    FILE LENGTH RETRIEVEMENT\n",
    "================================\n",
    "\n",
    "In this stage, the length of each file vector is collected and stored in a byte file.\n",
    "'''\n",
    "\n",
    "file_count = 2661\n",
    "def get_file_len():\n",
    "    global file_length\n",
    "    for item in tqdm(token_dic):\n",
    "        N_df = file_count/len(token_dic[item])\n",
    "        for doc_id in token_dic[item]:\n",
    "            _len = (1 + math.log10(token_dic[item][doc_id])) * math.log10(N_df)\n",
    "            if doc_id in file_length:\n",
    "                file_length[doc_id] += pow(_len, 2)\n",
    "            else:\n",
    "                file_length[doc_id] = pow(_len, 2)\n",
    "\n",
    "get_file_len()\n",
    "with open(os.path.join(BASE_PATH, f\"file_len\"), \"wb\") as _file:\n",
    "    pickle.dump(file_length, _file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "1244\n",
      "1325\n",
      "756\n",
      "556\n",
      "678\n",
      "205\n",
      "900\n",
      "2234\n",
      "2397\n",
      "1100\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0162f70bbe2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0m_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exitexit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         )\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "'''\n",
    "===========================\n",
    "    INVERTED INDEX TEST\n",
    "===========================\n",
    "\n",
    "This stage tests the constructed inverted index.\n",
    "'''\n",
    "\n",
    "def cosine_score(_input) -> dict():\n",
    "    score = dict()\n",
    "    query = dict(Counter(_input))\n",
    "    for item in query:\n",
    "        if item in token_dic:\n",
    "            N_df = file_count/len(token_dic[item])\n",
    "            for doc_id in token_dic[item]:\n",
    "                W_td = (1 + math.log10(token_dic[item][doc_id])) * math.log10(N_df)\n",
    "                W_tq = (1 + math.log10(query[item])) * math.log10(N_df)\n",
    "                if doc_id in score:\n",
    "                    score[doc_id] += (W_tq * W_td)\n",
    "                else:\n",
    "                    score[doc_id] = W_tq * W_td\n",
    "    for doc_id in score:\n",
    "        score[doc_id] /= math.sqrt(file_length[doc_id])\n",
    "    return score\n",
    "\n",
    "\n",
    "with open(os.path.join(BASE_PATH, f\"inverted_index\"), \"rb\") as _file:\n",
    "    token_dic = pickle.loads(_file.read())\n",
    "\n",
    "with open(os.path.join(BASE_PATH, f\"file_len\"), \"rb\") as _file:\n",
    "    file_length = pickle.loads(_file.read())\n",
    "\n",
    "while(True):\n",
    "    _input = str.split(input().lower())\n",
    "    if _input[0] == \"exitexit\":\n",
    "        exit()\n",
    "    result = cosine_score(_input)\n",
    "    for doc_id in sorted(result.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(doc_id[0])\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
